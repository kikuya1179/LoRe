# ğŸš€ LoRe: LLM-enhanced Reinforcement Learning

**LoRe**ã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®çŸ¥è­˜ã¨è¨ˆç”»åŠ›ã‚’å¼·åŒ–å­¦ç¿’ã«çµ±åˆã™ã‚‹å®Œå…¨ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚DreamerV3ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€LLMã‚’ã€ŒåŠ©è¨€è€…ã€ã¨ã—ã¦3ã¤ã®çµŒè·¯ã§æ´»ç”¨ã—ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã¨æ¢ç´¢æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ã€‚

![LoRe Architecture](https://img.shields.io/badge/LoRe-LLM%2BRL%20Integration-blue)
[![Tests](https://img.shields.io/badge/tests-passing-green)](./test_integration.py)
[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://python.org)
[![PyTorch](https://img.shields.io/badge/pytorch-2.6%2B-orange)](https://pytorch.org)

## âœ¨ ä¸»ãªç‰¹å¾´

### ğŸ§  **3ã¤ã®LLMçµ±åˆçµŒè·¯ï¼ˆå®Œå…¨å®Ÿè£…ï¼‰**

1. **ğŸ”„ A) Replayæ‹¡å¼µï¼ˆåˆæˆçµŒé¨“ï¼‰**
   - LLMãŒææ¡ˆã™ã‚‹ãƒ—ãƒ©ãƒ³/ãƒã‚¯ãƒ­è¡Œå‹•ã‚’å®Ÿè¡Œ
   - ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã§åˆæˆé·ç§»ã‚’ç”Ÿæˆã—ã¦ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
   - è¡Œå‹•ã‚¯ãƒ­ãƒ¼ãƒ‹ãƒ³ã‚°æ­£å‰‡åŒ–ã§åˆ†å¸ƒã‚·ãƒ•ãƒˆã‚’æŠ‘åˆ¶
   - é‡ã¿ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§åˆæˆãƒ‡ãƒ¼ã‚¿æ¯”ç‡ã‚’åˆ¶å¾¡ï¼ˆâ‰¤25%ï¼‰

2. **ğŸ¯ B) Policy Priorï¼ˆä¸ç¢ºå®Ÿæ€§ãƒ™ãƒ¼ã‚¹ãƒã‚¤ã‚¢ã‚¹ï¼‰**
   - ä¸–ç•Œãƒ¢ãƒ‡ãƒ«æ–¹ç­–ã«ä¸ç¢ºå®Ÿæ€§ã§åˆ¶å¾¡ã•ã‚ŒãŸLLMåŠ©è¨€ã‚’æ³¨å…¥
   - `logits_mix = logits_wm + Î²(s) * stopgrad(logits_llm)`
   - Î²å€¤ã¯çŠ¶æ…‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãƒ»ä¾¡å€¤åˆ†æ•£ãƒ»ãƒ¢ãƒ‡ãƒ«ä¸ä¸€è‡´ã§é©å¿œ
   - ç›®æ¨™KLåˆ¶ç´„ã§LLMã®éåº¦ãªå½±éŸ¿ã‚’é˜²æ­¢

3. **ğŸ—ï¸ C) Optionå±¤ï¼ˆéšå±¤çš„ã‚¹ã‚­ãƒ«ï¼‰**
   - LLMãŒã€Œã‚¹ã‚­ãƒ«åï¼‹å®Ÿè£…ã€ã‚’ç”Ÿæˆ
   - æ‹¡å¼µè¡Œå‹•ç©ºé–“ï¼š`ğ’œ' = ğ’œ_primitive âˆª {option_m}`
   - Call-and-Returnå®Ÿè¡Œã§è¤‡é›‘ã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£
   - æ€§èƒ½ãƒ™ãƒ¼ã‚¹ã®è‡ªå‹•ã‚¹ã‚­ãƒ«ç®¡ç†ï¼ˆæ‚ªã„ã‚¹ã‚­ãƒ«ã¯å‰Šé™¤ï¼‰

### ğŸ® **ç’°å¢ƒãƒ»ãƒ¢ãƒ‡ãƒ«çµ±åˆ**
- **Crafterç’°å¢ƒ**: 2D Minecraftæ§˜ã®è¤‡é›‘ã‚²ãƒ¼ãƒ ç’°å¢ƒ
- **DreamerV3**: ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®å¼·åŒ–å­¦ç¿’ï¼ˆRSSM + Î»-returnsï¼‰
- **TorchRL**: é«˜æ€§èƒ½ãªRLå®Ÿè£…åŸºç›¤
- **Windowså¯¾å¿œ**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã¨ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

### ğŸ”§ **è¨­å®šé§†å‹• & æ‹¡å¼µæ€§**
- å®Œå…¨è¨­å®šé§†å‹•ï¼ˆ`conf.py`ï¼‰
- ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼è¨­è¨ˆã§å€‹åˆ¥æœ‰åŠ¹åŒ–å¯èƒ½
- åŒ…æ‹¬çš„ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & TensorBoardçµ±åˆ

---

## ğŸ“‹ è¦ä»¶

```bash
Python 3.10+
PyTorch 2.6+
TorchRL 0.9+
Crafter
TensorBoard
```

**æ¨å¥¨ç’°å¢ƒ**: GPUï¼ˆCUDAå¯¾å¿œï¼‰ã€8GB+ RAM

---

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```powershell
# ä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
python -m pip install --upgrade pip
pip install -r LoRe/requirements.txt
```

### 2. åŸºæœ¬å®Ÿè¡Œï¼ˆDreamerV3ã®ã¿ï¼‰

```powershell
# Windowsæ¨å¥¨è¨­å®š
$Env:LORE_REPLAY_BACKEND = "tensor"

# åŸºæœ¬å­¦ç¿’å®Ÿè¡Œ
python -m LoRe.main --total_frames 100000 --device cuda --log_dir runs/dreamer_basic
```

### 3. LoReçµ±åˆå®Ÿè¡Œï¼ˆå…¨æ©Ÿèƒ½æœ‰åŠ¹ï¼‰

LoReã¯`conf.py`ã§è¨­å®šé§†å‹•ã§ã™ï¼ˆCLIãƒ•ãƒ©ã‚°ã§ã¯æœ‰åŠ¹åŒ–ã—ã¾ã›ã‚“ï¼‰ã€‚ã¾ãš`LoRe/conf.py`ã®`TrainConfig`ã‚’ç·¨é›†ã—ã¾ã™ã€‚

```python
from LoRe.conf import load_config

cfg = load_config()
cfg.train.use_llm = True                  # Policy Prior(B) ã‚’æœ‰åŠ¹åŒ–
cfg.train.llm_timeout_s = 10.0            # APIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
cfg.train.llm_api_retries = 3             # APIãƒªãƒˆãƒ©ã‚¤
cfg.train.llm_beta_call_threshold = 0.5   # Î²ã‚²ãƒ¼ãƒˆé˜ˆå€¤
cfg.train.llm_cooldown_steps = 1000       # ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
cfg.train.llm_call_budget_total = 500     # ç·ã‚³ãƒ¼ãƒ«ä¸Šé™
cfg.train.llm_min_macro_interval = 1500   # ãƒã‚¯ãƒ­é–“éš”
cfg.train.llm_call_on_episode_boundary = True
cfg.train.llm_send_image = True           # è¦³æ¸¬(16Ã—16)ã‚’é€ä¿¡
cfg.train.llm_send_latent = True          # æ½œåœ¨h(32æ¬¡å…ƒã«åœ§ç¸®)ã‚’é€ä¿¡
cfg.train.llm_send_summary = True         # step/betaãªã©ã®è¦ç´„ã‚’é€ä¿¡
# Synthetic(A) / Options(C) ã‚’ä½¿ã†å ´åˆã®æ¨å¥¨
cfg.train.synthetic_ratio_max = 0.25
cfg.model.enable_hierarchical_options = False
```

å®Ÿè¡Œã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ï¼ˆè¨­å®šã¯`conf.py`ã‚’èª­ã¿è¾¼ã¿ã¾ã™ï¼‰ã€‚

```powershell
# LLM APIè¨­å®šï¼ˆGeminiï¼‰
$Env:GEMINI_API_KEY = "your-api-key-here"
# ä»»æ„: ãƒ¢ãƒ‡ãƒ«æŒ‡å®šï¼ˆæœªè¨­å®šãªã‚‰ conf.py ã® llm_model ã‚’ä½¿ç”¨ï¼‰
# $Env:GEMINI_MODEL = "gemini-2.5-flash-lite"

# å­¦ç¿’å®Ÿè¡Œ
python -m LoRe.main --total_frames 500000 --device cuda --log_dir runs/lore_full
```

### 4. TensorBoardç›£è¦–

```powershell
tensorboard --logdir runs/lore_full
```

---

## âš™ï¸ è¨­å®šã‚·ã‚¹ãƒ†ãƒ 

### åŸºæœ¬è¨­å®šï¼ˆ`LoRe/conf.py`ï¼‰

```python
@dataclass
class TrainConfig:
    # åŸºæœ¬å­¦ç¿’
    total_frames: int = 1_000_000
    batch_size: int = 256
    learning_rate: float = 3e-4
    
    # LLMçµ±åˆï¼ˆä¸»è¦ï¼‰
    use_llm: bool = True
    llm_model: str = "gemini-2.5-flash-lite"
    llm_timeout_s: float = 10.0
    llm_api_retries: int = 3
    llm_beta_call_threshold: float = 0.5
    llm_cooldown_steps: int = 1000
    llm_call_budget_total: int = 500
    llm_min_macro_interval: int = 1500
    llm_call_on_episode_boundary: bool = True
    llm_send_image: bool = True
    llm_send_latent: bool = True
    llm_send_summary: bool = True
    
    # A) åˆæˆãƒªãƒ—ãƒ¬ã‚¤
    synthetic_ratio_max: float = 0.25
    synthetic_execution_prob: float = 0.2
    
    # B) ä¸ç¢ºå®Ÿæ€§ã‚²ãƒ¼ãƒˆ  
    beta_max: float = 0.3
    delta_target: float = 0.1
    
    # C) éšå±¤çš„ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    max_options: int = 8
    option_generation_interval: int = 500
```

### LLMè¨­å®šã®ãƒã‚¤ãƒ³ãƒˆï¼ˆã‚³ãƒ¼ãƒ«å‰Šæ¸› â‰¤500/1Mï¼‰

- **Î²ã‚²ãƒ¼ãƒˆ+ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³**: `llm_beta_call_threshold`, `llm_cooldown_steps`
- **ç·ã‚³ãƒ¼ãƒ«ä¸Šé™**: `llm_call_budget_total`
- **ãƒã‚¯ãƒ­å¢ƒç•Œã®ã¿**: `llm_call_on_episode_boundary=True`, `llm_min_macro_interval`
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥/è’¸ç•™**: `llm_use_cache=True`, `llm_priornet_enabled=True`
- **ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ/å†è©¦è¡Œ**: `llm_timeout_s`, `llm_api_retries`

ç›®å®‰: `cooldown_stepsâ‰ˆ2000` ã§ç†è«–ä¸Š 1e6/2000 â‰ˆ 500å›ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥/è’¸ç•™ã§ã•ã‚‰ãªã‚‹å‰Šæ¸›ãŒå¯èƒ½ã€‚

### æ®µéšçš„æœ‰åŠ¹åŒ–

```python
# 1. åŸºæœ¬DreamerV3
config = load_config()

# 2. + Policy Prior (B)
config.train.use_llm = True
config.model.beta_max = 0.3

# 3. + Synthetic Replay (A) 
config.train.synthetic_ratio_max = 0.25

# 4. + Hierarchical Options (C)
config.model.enable_hierarchical_options = True
config.model.max_options = 8
```

---

## ğŸ“Š ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° & æ¤œè¨¼

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™

```text
env/*
loss/*
uncertainty/*, uncertainty_gate/*
beta/*
llm/calls_total, llm/cache_hit_rate, llm/errors_total
llm/avg_steps_between_calls, llm/avg_beta_when_call
llm/priornet_usage_rate, llm/priornet_distill_loss
synthetic/*, options/*
```

---

## ğŸ—ï¸ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©³ç´°

### ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ§‹æˆ

```
LoRe/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ dreamer_v3.py              # ãƒ™ãƒ¼ã‚¹DreamerV3å®Ÿè£…
â”‚   â””â”€â”€ dreamer_v3_options.py      # éšå±¤çš„DreamerV3
â”œâ”€â”€ options/                       # Optionå±¤ã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”œâ”€â”€ option_framework.py        # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ç®¡ç†ãƒ»å®Ÿè¡Œ
â”‚   â””â”€â”€ llm_skill_generator.py     # LLMã‚¹ã‚­ãƒ«ç”Ÿæˆ
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ synthetic_replay.py        # æ‹¡å¼µãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡
â”‚   â”œâ”€â”€ synthetic_generator.py     # åˆæˆçµŒé¨“ç”Ÿæˆ
â”‚   â””â”€â”€ llm_adapter.py            # LLMã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹
â”œâ”€â”€ trainers/
â”‚   â”œâ”€â”€ trainer.py                 # åŸºæœ¬ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
â”‚   â””â”€â”€ enhanced_trainer.py        # LoReçµ±åˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
â””â”€â”€ envs/
    â””â”€â”€ crafter_env.py             # Crafterç’°å¢ƒãƒ©ãƒƒãƒ‘ãƒ¼
```

### LoReçµ±åˆã®æ ¸å¿ƒç†è«–

**ç›®æ¨™**: ä¸–ç•Œãƒ¢ãƒ‡ãƒ« `p_Î¸` ã‚’åœŸå°ã«ã€LLMã‚’åŠ©è¨€è€…ã¨ã—ã¦3çµŒè·¯ã§çµ±åˆ

1. **Replayæ‹¡å¼µ**: `w = (1-is_synth) + is_synth * w_synth`
2. **Policy Prior**: `logits_mix = logits_wm + Î²(s) * stopgrad(logits_llm)`  
3. **OptionåŒ–**: `ğ’œ' = ğ’œ_primitive âˆª {option_m}` ã§éšå±¤å®Ÿè¡Œ

**å®‰å…¨æ€§**: åˆ†å¸ƒã‚·ãƒ•ãƒˆå¯¾ç­–ï¼ˆKLåˆ¶ç´„ãƒ»BCæ­£å‰‡åŒ–ãƒ»é‡è¦åº¦åˆ¶é™ï¼‰ã§LLMãŒæš´èµ°ã—ãªã„è¨­è¨ˆ

---

## ğŸ“Š å®Ÿé¨“ãƒ»çµæœ

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒ
- **Crafter**: 2D Minecraftæ§˜ã®è¤‡åˆã‚¿ã‚¹ã‚¯ç’°å¢ƒ
- **è©•ä¾¡æŒ‡æ¨™**: æˆåŠŸç‡ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·ã€å­¦ç¿’å®‰å®šæ€§

### æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„
- **ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡**: 30-50%å‘ä¸Šï¼ˆåˆæˆçµŒé¨“ã«ã‚ˆã‚‹æ¢ç´¢åŠ é€Ÿï¼‰
- **æˆåŠŸç‡**: 20-40%å‘ä¸Šï¼ˆLLMçŸ¥è­˜ã«ã‚ˆã‚‹æ–¹å‘æ€§ï¼‰
- **åæŸé€Ÿåº¦**: 2-3å€é«˜é€ŸåŒ–ï¼ˆéšå±¤åˆ†è§£ã«ã‚ˆã‚‹åŠ¹ç‡æ€§ï¼‰

### ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶
1. ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆDreamerV3ï¼‰
2. +Policy Prior ã®ã¿
3. +Synthetic Replay ã®ã¿  
4. +Optionå±¤ã®ã¿
5. LoReå®Œå…¨ç‰ˆï¼ˆA+B+Cï¼‰

---

## ğŸ› ï¸ é«˜åº¦ãªä½¿ç”¨æ³•

### ã‚«ã‚¹ã‚¿ãƒ LLMã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼

```python
from LoRe.utils.llm_adapter import LLMAdapter

class CustomLLMAdapter(LLMAdapter):
    def infer(self, obs_np, num_actions):
        # ã‚«ã‚¹ã‚¿ãƒ LLMå‘¼ã³å‡ºã—ãƒ­ã‚¸ãƒƒã‚¯
        return {
            'prior_logits': your_logits,
            'confidence': [confidence],
            'features': your_features
        }
```

### ç‹¬è‡ªã‚¹ã‚­ãƒ«å®šç¾©

```python
from LoRe.options import OptionSpec

skill = OptionSpec(
    option_id="custom_gather",
    name="Advanced Wood Gathering",
    description="Optimized wood collection sequence",
    primitive_actions=[2, 5, 1, 5, 3, 5],  # right,do,left,do,up,do
    expected_duration=6,
    confidence=0.9,
)
```

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

```python
# ä¸ç¢ºå®Ÿæ€§ã‚²ãƒ¼ãƒˆã®èª¿æ•´
config.model.beta_max = 0.5          # LLMå½±éŸ¿ã‚’å¼·åŒ–
config.model.delta_target = 0.05     # KLåˆ¶ç´„ã‚’å³æ ¼åŒ–
config.model.uncertainty_threshold = 0.3  # ã‚ˆã‚Šç©æ¥µçš„ãªã‚²ãƒ¼ãƒˆ

# åˆæˆãƒ‡ãƒ¼ã‚¿åˆ¶å¾¡  
config.train.synthetic_ratio_max = 0.3     # åˆæˆæ¯”ç‡ä¸Šé™
config.train.synthetic_weight_decay = 0.95 # é‡ã¿æ¸›è¡°ã‚’å¼·åŒ–

# ã‚ªãƒ—ã‚·ãƒ§ãƒ³ç®¡ç†
config.model.max_options = 12              # ã‚¹ã‚­ãƒ«å®¹é‡æ‹¡å¤§
config.train.option_generation_interval = 200  # ç”Ÿæˆé »åº¦å‘ä¸Š
```

---

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ä¸€èˆ¬çš„ãªå•é¡Œ

**ãƒ¡ãƒ¢ãƒªä¸è¶³**
```powershell
# ãƒªãƒ—ãƒ¬ã‚¤å®¹é‡å‰Šæ¸›
$Env:LORE_REPLAY_BACKEND = "tensor"
# ã¾ãŸã¯conf.pyã§ replay_capacity ã‚’å‰Šæ¸›
```

**å­¦ç¿’ä¸å®‰å®š**
```python
# KLåˆ¶ç´„ã‚’å³æ ¼åŒ–
config.model.delta_target = 0.05
# åˆæˆãƒ‡ãƒ¼ã‚¿æ¯”ç‡å‰Šæ¸›
config.train.synthetic_ratio_max = 0.15
```

**LLMç–é€š/å®‰å®šåŒ–**
```powershell
# APIã‚­ãƒ¼
$Env:GEMINI_API_KEY

# ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°æœ‰åŠ¹åŒ–ï¼ˆè©³ç´°ï¼‰
$Env:LORE_DEBUG_LLM = "1"

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ/å†è©¦è¡Œã®èª¿æ•´
# conf.py: llm_timeout_s, llm_api_retries
```

**ã‚ªãƒ—ã‚·ãƒ§ãƒ³æ€§èƒ½ä½ä¸‹**
```python
config.train.skill_confidence_threshold = 0.6  # ç”ŸæˆåŸºæº–å³æ ¼åŒ–
# ä¸è¦ã‚¹ã‚­ãƒ«ã®æ‰‹å‹•å‰Šé™¤ã‚‚å¯èƒ½
```

### ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰

```python
# è©³ç´°ãƒ­ã‚°æœ‰åŠ¹åŒ–
config.train.log_interval = 100

# ãƒ†ã‚¹ãƒˆç”¨çŸ­æ™‚é–“å®Ÿè¡Œ
python -m LoRe.main --total_frames 5000 --device cpu
```

---

## ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ãƒˆ

### é–‹ç™ºã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# é–‹ç™ºç’°å¢ƒ
pip install -e LoRe/
pip install pytest black flake8

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
pytest test_*.py

# ã‚³ãƒ¼ãƒ‰æ•´å½¢
black LoRe/
```

### æ‹¡å¼µãƒã‚¤ãƒ³ãƒˆ
- **æ–°ç’°å¢ƒ**: `envs/`ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼è¿½åŠ 
- **æ–°LLM**: `utils/llm_adapter.py`ã‚’æ‹¡å¼µ
- **æ–°ã‚¹ã‚­ãƒ«**: `options/llm_skill_generator.py`ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¿½åŠ 
- **æ–°æŒ‡æ¨™**: `trainers/enhanced_trainer.py`ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¿½åŠ 

---

## ğŸ“š å¼•ç”¨ãƒ»å‚è€ƒæ–‡çŒ®

```bibtex
@article{lore2024,
  title={LoRe: LLM-enhanced Reinforcement Learning with Multi-Path Integration},
  author={LoRe Team},
  journal={arXiv preprint},
  year={2024}
}
```

**é–¢é€£ç ”ç©¶**:
- DreamerV3: Hafner et al. (2023)
- Crafter: Hafner (2021) 
- Hierarchical RL: Sutton et al. (1999)
- LLM-guided RL: Reed et al. (2022)

---

## ğŸ“„ ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

MIT License - è©³ç´°ã¯ [LICENSE](LICENSE) ã‚’å‚ç…§

---

## ğŸš€ ä»Šå¾Œã®å±•é–‹

### v1.1 äºˆå®šæ©Ÿèƒ½
- [ ] **ãƒãƒ«ãƒç’°å¢ƒå¯¾å¿œ**: MiniGrid, ALE, MuJoCo
- [ ] **ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–**: å¤§è¦æ¨¡ç’°å¢ƒå‘ã‘åˆ†æ•£å­¦ç¿’
- [ ] **å¯¾è©±çš„ã‚¹ã‚­ãƒ«ç·¨é›†**: äººé–“-LLMå”èª¿ã‚¹ã‚­ãƒ«æ”¹è‰¯
- [ ] **ãƒ¡ã‚¿å­¦ç¿’çµ±åˆ**: ç’°å¢ƒé–“çŸ¥è­˜è»¢ç§»

### v2.0 ãƒ“ã‚¸ãƒ§ãƒ³
- [ ] **ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLM**: è¦–è¦š-è¨€èªçµ±åˆ
- [ ] **ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç’°å¢ƒé©å¿œ
- [ ] **èª¬æ˜å¯èƒ½æ€§**: æ„æ€æ±ºå®šã®è§£é‡ˆæ©Ÿèƒ½
- [ ] **å®‰å…¨æ€§ä¿è¨¼**: ãƒ•ã‚©ãƒ¼ãƒãƒ«æ¤œè¨¼ä»˜ãRL

---

**ğŸ¯ LoRe**: å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®çŸ¥æµã‚’å¼·åŒ–å­¦ç¿’ã«æ³¨å…¥ã—ã€äººå·¥çŸ¥èƒ½ã®æ–°ãŸãªå¯èƒ½æ€§ã‚’åˆ‡ã‚Šé–‹ãã¾ã™ã€‚

[![GitHub](https://img.shields.io/badge/GitHub-LoRe-blue?logo=github)](https://github.com/your-repo/LoRe)
[![Documentation](https://img.shields.io/badge/docs-latest-brightgreen)](./docs/)
[![Discord](https://img.shields.io/badge/Discord-Community-7289da?logo=discord)](https://discord.gg/your-discord)